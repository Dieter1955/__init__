# Introduction
MLT \_\_init\_\_ is a monthly event led by [Jayson](https://www.linkedin.com/in/jayson-cunanan-phd/) and [Miguel](https://twitter.com/jmlipman) where, similarly to a traditional journal club, a paper is first presented by a volunteer and then discussed with the collaboration of the audience. Our goal is to give participants good **initializations** to effectively study and improve their understanding of Deep Learning. We will try to achieve this by:
* Discussing **fundamental papers**, whose main ideas are currently implemented on state-of-the-art models.
* Providing the audience with summaries, codes and visualizations to help understand the critical parts of a paper.

# Sessions
| Date        | Topic                           | Paper                  | Presenter          | Materials | Video |
|-------------|---------------------------------|------------------------|--------------------|--------------------|--------------------|
| 10/Jan/2021 | CV: Separable Convolutions      | [Xception](https://arxiv.org/abs/1610.02357)               | [Jayson Cunanan](https://www.linkedin.com/in/jayson-cunanan-phd/)     | [Materials](https://github.com/Machine-Learning-Tokyo/__init__) | [![Youtube](https://www.youtube.com/s/desktop/a386e432/img/favicon_32.png)](https://youtube.com) |
| 14/Feb/2021 | CV: Dilated Convolutions + ASPP | [DeepLabv2](https://arxiv.org/abs/1606.00915)              | [J. Miguel Valverde](https://www.twitter.com/jmlipman)    |  [Materials](https://github.com/Machine-Learning-Tokyo/__init__) | [![Youtube](https://www.youtube.com/s/desktop/a386e432/img/favicon_32.png)](https://youtube.com) |
| 14/Mar/2021 | CV: Attention in Images         | [Squeeze and Excitation](https://arxiv.org/abs/1709.01507) | [Alisher Abdulkhaev](https://twitter.com/alisher_ai) |  [Materials](https://github.com/Machine-Learning-Tokyo/__init__) | [![Youtube](https://www.youtube.com/s/desktop/a386e432/img/favicon_32.png)](https://youtube.com) |
| 11/Apr/2021 | CV: Attention in GANs | [Self-Attention Generative Adversarial Networks](https://arxiv.org/abs/1805.08318) | [Mayank Bhaskar](https://twitter.com/cataluna84) |  |  |
| 9/May/2021 | NLP: Attention | [Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078) | Ana Valeria  |  |  |
| 13/Jun/2021 | NLP: Attention | [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) | Charles Melby-Thompson |  |  |


Sessions will be held via Zoom starting at 5pm (JST) / 9am (CET). Check at what time is in your region [here](https://www.worldtimebuddy.com/japan-tokyo-to-cet).

## Format
Introduction (5min) + Paper presentation (25min) + Discussion (60min)

The introduction and paper presentation will be recorded (if agreed with the presenter) whereas the discussion will not be recorded. This format allows participants who want to keep their privacy a space to ask questions after the presentation.

## For participants
As most of the time is allocated for discussion, we kindly ask participants to read the papers in advance and to come to the session with two questions or comments in mind. These questions/comments can be to highlight interesting or unclear parts: What did you like the most about this paper? What did you learn? What did you not understand?

The participants are encouraged to ask questions even in the middle of the presentation to make the discussion more interactive. Therefore, we kindly ask participants to keep in mind the environmental noise. If you cannot use your microphone or you want to keep your privacy during the presentation, you are welcome to write in the Zoom chat or Slack channel.


## For presenters
Each presenter can use their favorite app to deliver the presentation during the online meeting. For code sharing, please use Jupyter notebooks so others can easily use Google Colab to experiment on your code. 

## Code of Conduct
Remember to be kind and respectful to each other. Full Code of Conduct [here](https://mltokyo.ai/about).

Feedback/Contact form: https://forms.gle/jJLWyAMjjVKL8KFRA
